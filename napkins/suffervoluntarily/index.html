<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="Chris Kerwell Gresla ">
<meta name="description" content="​
Context I have spent the last few weeks immersing myself in the Policy Optimization Literature, within the larger field of Reinforcement Learning. This branch of Machine Learning has always been my &amp;ldquo;fancy&amp;rdquo;, of the main fields in ML, RL is the most &amp;ldquo;gangster&amp;rdquo;. This affinity for the field of figuring out how to create Agents that can learn things more closely to the way that humans learn is what has prompted me to wake up a little extra early each day; diving into papers and programming before the hitting the gym and before the main work day." />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://tonkatsu.io/napkins/suffervoluntarily/" />


    <title>
        
            Why Voluntary Suffering is Worth it :: Chris Kerwell Gresla 
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="https://tonkatsu.io/main.06a7877229b0dcfce947ab9f7912b5198411d0c19033b4bab491889f84373e2d.css">



    <link rel="apple-touch-icon" sizes="180x180" href="https://tonkatsu.io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://tonkatsu.io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://tonkatsu.io/favicon-16x16.png">
    <link rel="manifest" href="https://tonkatsu.io/site.webmanifest">
    <link rel="mask-icon" href="https://tonkatsu.io/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="https://tonkatsu.io/favicon.ico">
    <meta name="msapplication-TileColor" content="">
    <meta name="theme-color" content="">



<meta itemprop="name" content="Why Voluntary Suffering is Worth it">
<meta itemprop="description" content="​
Context I have spent the last few weeks immersing myself in the Policy Optimization Literature, within the larger field of Reinforcement Learning. This branch of Machine Learning has always been my &ldquo;fancy&rdquo;, of the main fields in ML, RL is the most &ldquo;gangster&rdquo;. This affinity for the field of figuring out how to create Agents that can learn things more closely to the way that humans learn is what has prompted me to wake up a little extra early each day; diving into papers and programming before the hitting the gym and before the main work day."><meta itemprop="datePublished" content="2022-10-09T00:00:00+00:00" />
<meta itemprop="dateModified" content="2022-10-09T00:00:00+00:00" />
<meta itemprop="wordCount" content="765"><meta itemprop="image" content="https://tonkatsu.io"/>
<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://tonkatsu.io"/>

<meta name="twitter:title" content="Why Voluntary Suffering is Worth it"/>
<meta name="twitter:description" content="​
Context I have spent the last few weeks immersing myself in the Policy Optimization Literature, within the larger field of Reinforcement Learning. This branch of Machine Learning has always been my &ldquo;fancy&rdquo;, of the main fields in ML, RL is the most &ldquo;gangster&rdquo;. This affinity for the field of figuring out how to create Agents that can learn things more closely to the way that humans learn is what has prompted me to wake up a little extra early each day; diving into papers and programming before the hitting the gym and before the main work day."/>




    <meta property="og:title" content="Why Voluntary Suffering is Worth it" />
<meta property="og:description" content="​
Context I have spent the last few weeks immersing myself in the Policy Optimization Literature, within the larger field of Reinforcement Learning. This branch of Machine Learning has always been my &ldquo;fancy&rdquo;, of the main fields in ML, RL is the most &ldquo;gangster&rdquo;. This affinity for the field of figuring out how to create Agents that can learn things more closely to the way that humans learn is what has prompted me to wake up a little extra early each day; diving into papers and programming before the hitting the gym and before the main work day." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tonkatsu.io/napkins/suffervoluntarily/" /><meta property="og:image" content="https://tonkatsu.io"/><meta property="article:section" content="napkins" />
<meta property="article:published_time" content="2022-10-09T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-10-09T00:00:00+00:00" />







    <meta property="article:published_time" content="2022-10-09 00:00:00 &#43;0000 UTC" />










    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="https://tonkatsu.io/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark"> </span>
            <span class="logo__text">♥</span>
            <span class="logo__cursor" style=
                  "visibility:hidden;
                   background-color:#67a2c9;
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://tonkatsu.io/about/">About</a></li><li><a href="https://tonkatsu.io/napkins/">Napkins</a></li><li><a href="https://tonkatsu.io/ponderings/">Ponderings</a></li><li><a href="https://tonkatsu.io/quips/">Quips</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title"><a href="https://tonkatsu.io/napkins/suffervoluntarily/">Why Voluntary Suffering is Worth it</a></h2>

            
            
            

            <div class="post-content">
                <p>​</p>
<h4 id="context">Context</h4>
<p>I have spent the last few weeks immersing myself in the <a href="https://github.com/ckgresla/MI6/blob/main/intel/policy_gradients.md">Policy Optimization Literature</a>, within the larger field of <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement Learning</a>. This branch of Machine Learning has always been my &ldquo;fancy&rdquo;, of the main fields in ML, RL is the most &ldquo;gangster&rdquo;. This affinity for the field of figuring out how to create Agents that can learn things more closely to the way that humans learn is what has prompted me to wake up a little extra early each day; diving into papers and programming before the hitting the gym and before the main work day.</p>
<p>These past 2-ish weeks have been spent trying to implement the VPG, basically what was done in <a href="https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf">Sutton&rsquo;s paper from 2000</a> with the addition of generalized advantage estimation (<a href="http://joschu.net/docs/thesis.pdf">see section 4</a>) a method for reducing variance of the Policy Gradient (effectively the &ldquo;error&rdquo; we want to backprop to our Policy Network parameters for learning).</p>
<p>​</p>
<h4 id="just-sit-with-it">Just Sit With it</h4>
<p>The math behind the Generalized Advantage Estimate (GAE) calculation isn&rsquo;t all that bad, Schulman (who is just brilliant) breaks everything down nicely in his thesis, the intuition for GAE is also a nice one; basically we want to train our Policy Network based on not all of the Reward signals but only the reward signals that come from Actions that do <em>better than expected</em>. It doesn&rsquo;t sound all that bad and the only difference from my prior implmentation of REINFORCE is just; the introduction a Value Function estimator (second network that is trained to predict reward given states) and calculating advantage estimates for backprop in the Policy Network.</p>
<p>Sounds trivial, right?</p>
<p>It actually is trivial! But <strong>never</strong> underestimate your ability to be an absolute nincompoop!</p>
<p>This problem really haunted me, it was one of those kinds of things where you understand it conceptually but looking at other implementations throws off your understanding and you get caught in this loop of trying to get it to work but then second guessing your understanding and reviewing the papers and math and blah blah blah. Everyone person that does difficult things occasionally finds themself in the &ldquo;spinning of the wheels&rdquo;, that lovely place amongst the mud, nonsense and the sense that they should stop being; [stupid, weak, a loser, dumb, lazy, etc.] and just get it done. In these times even though you do step away and let the problem marinate you just gotta sit with the problem. Progress is still made even if code/whatever your metric of performance isn&rsquo;t being increased. The mere <strong>act</strong> of <strong>enduring the muck</strong> increases your skills. Being stubborn and hitting your head against the wall or continously attempting to climb the plateau after falling is the way through. It sucks yea, and all that comes with sucking generally sucks; but the thing to remember is that it&rsquo;ll pass, and with each blow you take, you also get <strong>STRONGER</strong>.</p>
<p>So just sit with the problem, don&rsquo;t run the same function wondering why it doesn&rsquo;t work (or at least try to refrain from doing it &lt; 3 times before thinking about the error) &ndash; the answer will come in due time, you can think of it as your Unconscious Self <em>updating the gradient</em>.</p>
<p>​</p>
<h4 id="lots-of-ecstatic-yelling--some-jumping">Lots of Ecstatic Yelling &amp; Some Jumping</h4>
<p>After weeks of this debauchery, I sat down this lovely Autumn Sunday to do battle yet again with this. I spent the better part of an afternoon implementing GAE in my VPG with no luck.</p>
<ul>
<li>I tried the discounted cumsum trick, grads didn&rsquo;t get tracked correctly</li>
<li>I tried to do the compute with vectors as opposed to elementwise, issues with the discount cumsum calculation</li>
<li>I tried to do everything elementwise, Tensor has no attribute &ldquo;backwards()&rdquo;</li>
</ul>
<p>&hellip;</p>
<p>&hellip;.</p>
<p>&hellip;..</p>
<p>&hellip;&hellip;</p>
<p>WHAT DO YOU MEAN TENSOR HAS NO ATTRIBUTE BACKWARDS!</p>
<p>THE THING HAS A DIRECT LINE OF COMPUTATIONAL BACK TO THE PI NETWORK!</p>
<p>ugh</p>
<p>* ctrl+c, ctrl+v, enter, click stackoverflow *</p>
<p>wait&hellip; no</p>
<p>* checks torch documentation *</p>
<p>no no no no no</p>
<p>* changes call from loss.backwards() to loss.backward() *</p>
<p>oh god.</p>
<p>* switches tmux windows and runs the vpg file *</p>
<p>* it starts learning, and learning fast *</p>
<p>YO KJHGFAKJSDL HFKLSDHFJESIOUEARH ARE YOU F***ING SERIOSANJDKSHF SDAEJFGIOUWERHTWEHAFJSDKJHF KJAHFJSEK</p>
<p>* happiness *</p>
<p>It is good to remember that you are and have the capacity to be a real idiot &ndash; it is useful to assume this is the true State as it keeps you from being to arrogant and in the pursuit of knowledge.</p>
<p>It is also good, perhaps better, to remember that if you take the time to suffer voluntarily, doing something challenging, you are awesome.</p>

            </div>
        </article>

        <hr />

        <div class="post-info">
            
            
  		</div>
    </main>

            </div>

            
                <footer class="footer">
    
    
</footer>

            
        </div>

        



<script type="text/javascript" src="https://tonkatsu.io/bundle.min.cf7fb2a9e24608a783e05e4472da2fdf008293289de1ad88d1cba5c143e7e414496dd33b6a228e92f8461ce0c8cbad3c9f9847e73e46dfbce0463393ddd1733a.js" integrity="sha512-z3&#43;yqeJGCKeD4F5Ectov3wCCkyid4a2I0culwUPn5BRJbdM7aiKOkvhGHODIy608n5hH5z5G37zgRjOT3dFzOg=="></script>



    </body>
</html>
